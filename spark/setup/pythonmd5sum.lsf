#!/bin/sh
#BSUB -J pythonmd5sum
#BSUB -W 00:10 
#BSUB -oo pythonmd5sum_%J.log

if [[ -z $SPARK_HOME ]];then
export SPARK_HOME=$HOME/software/spark/1.5.0
fi

# initialize the nodes
python start_spark_lsf.py -m 2G -s /scratch/users/$USER

# creates the slaves file, starts the spark master and worker processes using mpirun
# the "-c" option specifies number of cores per worker
# -m specifies SPARK_MEMORY

# the specific example runs spark's pi estimation with a slices = 100 (first and only argument)

set +x
$SPARK_HOME/bin/spark-submit --master spark://$HOSTNAME:7077 --total-executor-cores ${LSB_DJOB_NUMPROC} drive_md5sum.py /projects/hpcsupport/steinbac/multiview_data/celegans_cube128_offsetsdefault
set -x

SPARK_SLAVES=/scratch/users/$USER/slaves_$LSB_JOBID
$SPARK_HOME/sbin/stop-all.sh
$SPARK_HOME/sbin/stop-slaves.sh
echo "cleanup slaves file $SPARK_SLAVES"
rm -v $SPARK_SLAVES
