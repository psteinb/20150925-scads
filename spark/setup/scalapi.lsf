#!/bin/sh
#BSUB -J sparkpi_scala
#BSUB -W 00:10 # requesting 10 minutes
#BSUB -oo scalapi_singlenode.log # output extra o means overwrite
#BSUB -eo scalapi_singlenode.err
#BSUB -n 12 # requesting 12 cores 
#BSUB -R span[hosts=1]

# module load new
# module load java
module load openmpi/1.8.6

if [[ -z $SPARK_HOME ]];then
export SPARK_HOME=$HOME/software/spark/1.4.1
fi

# initialize the nodes
python start_spark_madmax.py -c 12 #-m 48g 

# creates the slaves file, starts the spark master and worker processes using mpirun
# the "-c" option specifies number of cores per worker
# -m specifies SPARK_MEMORY

# the specific example runs spark's pi estimation with a slices = 100 (first and only argument)

echo " Master is set as $HOSTNAME"
JARFILE="$SPARK_HOME/lib/spark-examples*.jar" # version depends on spark version

$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi \
    --master spark://$HOSTNAME:7077 \
    --total-executor-cores 12 \
    $JARFILE 1000

SPARK_SLAVES=$HOME/slaves_$LSB_JOBID
$SPARK_HOME/sbin/stop-all.sh
$SPARK_HOME/sbin/stop-slaves.sh
